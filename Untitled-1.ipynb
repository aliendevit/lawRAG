{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8252cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.12.12\n",
      "cuda available: True\n",
      "device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch, platform, os\n",
    "print(\"python:\", platform.python_version())\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e72f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install sentence-transformers numpy\n",
    "# optional:\n",
    "!pip -q install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0606be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lawRAG'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 33 (delta 1), reused 33 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (33/33), 15.65 KiB | 7.82 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "/content/lawRAG\n",
      "Cloning into 'data_docs/raw/gesetze-master'...\n",
      "remote: Enumerating objects: 17193, done.\u001b[K\n",
      "remote: Counting objects: 100% (17193/17193), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10032/10032), done.\u001b[K\n",
      "remote: Total 17193 (delta 1064), reused 15638 (delta 987), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (17193/17193), 293.09 MiB | 16.36 MiB/s, done.\n",
      "Resolving deltas: 100% (1064/1064), done.\n",
      "Updating files: 100% (10709/10709), done.\n",
      "exists True\n",
      "md_count 6637\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aliendevit/lawRAG.git\n",
    "%cd lawRAG\n",
    "!mkdir -p data_docs/raw\n",
    "!git clone --depth 1 https://github.com/bundestag/gesetze.git data_docs/raw/gesetze-master\n",
    "!python -c \"from pathlib import Path; p=Path('data_docs/raw/gesetze-master'); print('exists', p.exists()); print('md_count', sum(1 for f in p.rglob('*.md')))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225b16e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/lawRAG\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | RUNNING FILE: /content/lawRAG/src/legalrag/ingest.py\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | RAW_MD_DIR(raw)='data_docs/raw/gesetze-master' | abs=/content/lawRAG/data_docs/raw/gesetze-master | exists=True\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | Loaded md_files from filelist: data_docs/processed/md_filelist.txt | count=6637\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | Sample md: /content/lawRAG/data_docs/raw/gesetze-master/README.md\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | After exclude sample md: /content/lawRAG/data_docs/raw/gesetze-master/README.md\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | RAW_MD_DIR=data_docs/raw/gesetze-master | md_files=6637\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | After exclude filter: md_files=6636\n",
      "2026-01-12 22:44:41 | INFO | legalrag.ingest | After exclude sample md: /content/lawRAG/data_docs/raw/gesetze-master/w/wzg_4appabek/index.md\n",
      "2026-01-12 22:44:42 | INFO | legalrag.ingest | Progress: 200/6636 files | sources=200 chunks=4607\n",
      "2026-01-12 22:44:42 | INFO | legalrag.ingest | Progress: 400/6636 files | sources=400 chunks=9792\n",
      "2026-01-12 22:44:43 | INFO | legalrag.ingest | Progress: 800/6636 files | sources=800 chunks=23318\n",
      "2026-01-12 22:44:44 | INFO | legalrag.ingest | Progress: 1000/6636 files | sources=1000 chunks=30898\n",
      "2026-01-12 22:44:46 | INFO | legalrag.ingest | Progress: 1400/6636 files | sources=1400 chunks=52333\n",
      "2026-01-12 22:44:47 | INFO | legalrag.ingest | Progress: 1600/6636 files | sources=1600 chunks=60486\n",
      "2026-01-12 22:44:48 | INFO | legalrag.ingest | Progress: 1800/6636 files | sources=1800 chunks=73722\n",
      "2026-01-12 22:44:49 | INFO | legalrag.ingest | Progress: 2200/6636 files | sources=2200 chunks=83967\n",
      "2026-01-12 22:44:50 | INFO | legalrag.ingest | Progress: 2400/6636 files | sources=2400 chunks=90876\n",
      "2026-01-12 22:44:51 | INFO | legalrag.ingest | Progress: 2600/6636 files | sources=2600 chunks=103669\n",
      "2026-01-12 22:44:51 | INFO | legalrag.ingest | Progress: 2800/6636 files | sources=2800 chunks=109808\n",
      "2026-01-12 22:44:52 | INFO | legalrag.ingest | Progress: 3000/6636 files | sources=3000 chunks=115419\n",
      "2026-01-12 22:44:54 | INFO | legalrag.ingest | Progress: 3800/6636 files | sources=3800 chunks=135461\n",
      "2026-01-12 22:44:54 | INFO | legalrag.ingest | Progress: 4000/6636 files | sources=4000 chunks=141435\n",
      "2026-01-12 22:44:55 | INFO | legalrag.ingest | Progress: 4200/6636 files | sources=4200 chunks=149583\n",
      "2026-01-12 22:44:55 | INFO | legalrag.ingest | Progress: 4400/6636 files | sources=4400 chunks=156111\n",
      "2026-01-12 22:44:56 | INFO | legalrag.ingest | Progress: 4600/6636 files | sources=4600 chunks=160387\n",
      "2026-01-12 22:44:56 | INFO | legalrag.ingest | Progress: 4800/6636 files | sources=4800 chunks=164924\n",
      "2026-01-12 22:44:57 | INFO | legalrag.ingest | Progress: 5000/6636 files | sources=5000 chunks=171029\n",
      "2026-01-12 22:44:58 | INFO | legalrag.ingest | Progress: 5200/6636 files | sources=5200 chunks=182522\n",
      "2026-01-12 22:45:01 | INFO | legalrag.ingest | Progress: 6000/6636 files | sources=6000 chunks=215950\n",
      "2026-01-12 22:45:02 | INFO | legalrag.ingest | Progress: 6200/6636 files | sources=6200 chunks=223492\n",
      "2026-01-12 22:45:03 | INFO | legalrag.ingest | Progress: 6400/6636 files | sources=6400 chunks=231066\n",
      "2026-01-12 22:45:04 | INFO | legalrag.ingest | Ingestion complete | sources=6636 chunks=238737 | seconds=22.3\n",
      "2026-01-12 22:45:11 | INFO | legalrag.index | Loaded JSONL chunks=238737 (bad=0)\n",
      "2026-01-12 22:45:11 | INFO | legalrag.index | Loaded chunks: 238737\n",
      "2026-01-12 22:45:29 | INFO | legalrag.index | BM25 built in 18.60s\n",
      "2026-01-12 22:45:36 | INFO | legalrag.index | Doc store saved: data_indexes/doc_store.jsonl\n",
      "2026-01-12 22:45:42 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
      "2026-01-12 22:45:50.404479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768257950.424164    1931 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768257950.430140    1931 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768257950.445019    1931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768257950.445043    1931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768257950.445047    1931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768257950.445052    1931 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-12 22:45:50.449556: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-12 22:45:56 | INFO | datasets | TensorFlow version 2.19.0 available.\n",
      "2026-01-12 22:45:56 | INFO | datasets | JAX version 0.7.2 available.\n",
      "2026-01-12 22:45:58 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cuda:0\n",
      "2026-01-12 22:45:58 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "modules.json: 100% 229/229 [00:00<00:00, 2.08MB/s]\n",
      "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 773kB/s]\n",
      "README.md: 3.89kB [00:00, 20.2MB/s]\n",
      "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 396kB/s]\n",
      "config.json: 100% 645/645 [00:00<00:00, 5.87MB/s]\n",
      "model.safetensors: 100% 471M/471M [00:03<00:00, 146MB/s]  \n",
      "tokenizer_config.json: 100% 480/480 [00:00<00:00, 4.80MB/s]\n",
      "tokenizer.json: 100% 9.08M/9.08M [00:00<00:00, 18.1MB/s]\n",
      "special_tokens_map.json: 100% 239/239 [00:00<00:00, 2.43MB/s]\n",
      "config.json: 100% 190/190 [00:00<00:00, 2.03MB/s]\n",
      "Batches: 100% 3731/3731 [07:04<00:00,  8.80it/s]\n",
      "2026-01-12 22:53:23 | INFO | legalrag.index | Embeddings saved.\n",
      "2026-01-12 22:53:23 | WARNING | legalrag.index | FAISS unavailable: No module named 'faiss'\n"
     ]
    }
   ],
   "source": [
    "%cd /content/lawRAG\n",
    "!pip -q install fastapi uvicorn rank-bm25 sentence-transformers numpy\n",
    "!python -m src.legalrag.ingest\n",
    "!python -m src.legalrag.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef06ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lawRAG'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 33 (delta 1), reused 33 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (33/33), 15.65 KiB | 15.65 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "Cloning into 'gesetze'...\n",
      "remote: Enumerating objects: 17193, done.\u001b[K\n",
      "remote: Counting objects: 100% (17193/17193), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10032/10032), done.\u001b[K\n",
      "remote: Total 17193 (delta 1064), reused 15638 (delta 987), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (17193/17193), 293.09 MiB | 17.19 MiB/s, done.\n",
      "Resolving deltas: 100% (1064/1064), done.\n",
      "Updating files: 100% (10709/10709), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aliendevit/lawRAG.git\n",
    "!git clone --depth 1 https://github.com/bundestag/gesetze.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baba6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/lawRAG\n",
      "Cloning into 'data_docs/raw/gesetze-master'...\n",
      "remote: Enumerating objects: 17193, done.\u001b[K\n",
      "remote: Counting objects: 100% (17193/17193), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10032/10032), done.\u001b[K\n",
      "remote: Total 17193 (delta 1064), reused 15638 (delta 987), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (17193/17193), 293.09 MiB | 16.39 MiB/s, done.\n",
      "Resolving deltas: 100% (1064/1064), done.\n",
      "Updating files: 100% (10709/10709), done.\n"
     ]
    }
   ],
   "source": [
    "%cd /content/lawRAG\n",
    "!mkdir -p data_docs/raw\n",
    "!rm -rf data_docs/raw/gesetze-master\n",
    "!git clone --depth 1 https://github.com/bundestag/gesetze.git data_docs/raw/gesetze-master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "222ca3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_docs/raw/gesetze-master/w/wzg_4appabek/index.md\n",
      "data_docs/raw/gesetze-master/w/wzg_35brbbek/index.md\n",
      "data_docs/raw/gesetze-master/w/wpostvtr1989g/index.md\n",
      "data_docs/raw/gesetze-master/w/wzg_4espbek/index.md\n",
      "data_docs/raw/gesetze-master/w/wsvzustneuov/index.md\n",
      "data_docs/raw/gesetze-master/w/wostatg/index.md\n",
      "data_docs/raw/gesetze-master/w/wpschchegdv/index.md\n",
      "data_docs/raw/gesetze-master/w/wvgeflpestmonv/index.md\n",
      "data_docs/raw/gesetze-master/w/wzg_35thabek/index.md\n",
      "data_docs/raw/gesetze-master/w/wzg_4wienbek/index.md\n",
      "exists True\n",
      "md 6637\n"
     ]
    }
   ],
   "source": [
    "!find data_docs/raw/gesetze-master -type f -name \"*.md\" | head\n",
    "!python -c \"from pathlib import Path; root=Path('data_docs/raw/gesetze-master'); print('exists', root.exists()); print('md', sum(1 for p in root.rglob('*.md')))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170016d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/lawRAG\n"
     ]
    }
   ],
   "source": [
    "%cd /content/lawRAG\n",
    "!cp -f .env.example .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e743dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Optional overrides (defaults already set in src/legalrag/config.py)\n",
      "# RAW_MD_DIR=data_docs/raw/gesetze_md\n",
      "# PROCESSED_DIR=data_docs/processed\n",
      "# INDEX_DIR=data_indexes\n",
      "# RETRIEVAL_MODE=hybrid\n",
      "# TOP_K=8\n",
      "\n",
      "RAW_MD_DIR=data_docs/raw/gesetze-master\n",
      "PROCESSED_DIR=data_docs/processed\n",
      "INDEX_DIR=data_indexes\n",
      "RETRIEVAL_MODE=bm25\n"
     ]
    }
   ],
   "source": [
    "!printf \"\\nRAW_MD_DIR=data_docs/raw/gesetze-master\\nPROCESSED_DIR=data_docs/processed\\nINDEX_DIR=data_indexes\\nRETRIEVAL_MODE=bm25\\n\" >> .env\n",
    "!tail -n 20 .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f317e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | RUNNING FILE: /content/lawRAG/src/legalrag/ingest.py\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | RAW_MD_DIR(raw)='data_docs/raw/gesetze-master' | abs=/content/lawRAG/data_docs/raw/gesetze-master | exists=True\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | Loaded md_files from filelist: data_docs/processed/md_filelist.txt | count=6637\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | Sample md: /content/lawRAG/data_docs/raw/gesetze-master/README.md\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | After exclude sample md: /content/lawRAG/data_docs/raw/gesetze-master/README.md\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | RAW_MD_DIR=data_docs/raw/gesetze-master | md_files=50\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | After exclude filter: md_files=49\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | After exclude sample md: /content/lawRAG/data_docs/raw/gesetze-master/w/wzg_4appabek/index.md\n",
      "2026-01-12 22:55:45 | INFO | legalrag.ingest | Ingestion complete | sources=49 chunks=1162 | seconds=0.1\n",
      "1162 data_docs/processed/statute_chunks.jsonl\n",
      "{\"chunk_id\": \"STATUTE::WPostVtr1989G::Art.1::AbsNA::6e5e532e::1\", \"doc_type\": \"statute\", \"law\": {\"title\": \"Gesetz zu den Verträgen vom 14. Dezember 1989 des Weltpostvereins (WPostVtr1989G)\", \"short\": \"WPostVtr1989G\", \"jurisdiction\": \"DE-BUND\"}, \"provision\": {\"kind\": \"Art.\", \"num\": \"1\", \"norm_title\": \"\", \"absatz\": null, \"satz\": null}, \"citation\": {\"canonical\": \"WPostVtr1989G Art. 1\", \"locator\": {\"gesetz\": \"WPostVtr1989G\", \"artikel\": \"1\", \"absatz\": null, \"satz\": null}}, \"text\": \"Den folgenden in Washington am 14. Dezember 1989 von der\\nBundesrepublik Deutschland unterzeichneten Verträgen des\\nWeltpostvereins\\n\\n1.  Viertes Zusatzprotokoll zur Satzung des Weltpostvereins\\n\\n\\n2.  Allgemeine Verfahrensordnung des Weltpostvereins nebst Anhang\\n\\n\\n3.  Weltpostvertrag\\n\\n\\n4.  Postpaketabkommen\\n\\n\\n5.  Postanweisungsabkommen\\n\\n\\n6.  Postgiroabkommen und\\n\\n\\n7.  Postnachnahmeabkommen\\n\\n\\n\\nnebst Schlußprotokollen wird zugestimmt. Die Verträge nebst\\nSchlußprotokollen werden nachstehend veröffentlicht.\", \"snippets\": [{\"snippet_id\": \"STATUTE::WPostVtr1989G::Art.1::AbsNA::6e5e532e::1::S1\", \"text\": \"Den folgenden in Washington am 14.\", \"span\": {\"start\": 0, \"end\": 34}}, {\"snippet_id\": \"STATUTE::WPostVtr1989G::Art.1::AbsNA::6e5e532e::1::S2\", \"text\": \"Dezember 1989 von der\\nBundesrepublik Deutschland unterzeichneten Verträgen des\\nWeltpostvereins\", \"span\": {\"start\": 35, \"end\": 129}}, {\"snippet_id\": \"STATUTE::WPostVtr1989G::Art.1::AbsNA::6e5e532e::1::S3\", \"text\": \"1.\", \"span\": {\"start\": 131, \"end\": 133}}], \"source\": {\"source_id\": \"SRC::gesetze::index.md::6e5e532e2802\", \"path\": \"/content/lawRAG/data_docs/raw/gesetze-master/w/wpostvtr1989g/index.md\", \"sha256\": \"6e5e532e2802d826ab3b24b152eda7c8140913ede9596cf5c98bb331ded30865\"}}\n",
      "{\"chunk_id\": \"STATUTE::WPostVtr1989G::Art.2::AbsNA::6e5e532e::2\", \"doc_type\": \"statute\", \"law\": {\"title\": \"Gesetz zu den Verträgen vom 14. Dezember 1989 des Weltpostvereins (WPostVtr1989G)\", \"short\": \"WPostVtr1989G\", \"jurisdiction\": \"DE-BUND\"}, \"provision\": {\"kind\": \"Art.\", \"num\": \"2\", \"norm_title\": \"\", \"absatz\": null, \"satz\": null}, \"citation\": {\"canonical\": \"WPostVtr1989G Art. 2\", \"locator\": {\"gesetz\": \"WPostVtr1989G\", \"artikel\": \"2\", \"absatz\": null, \"satz\": null}}, \"text\": \"Der Bundesminister für Post und Telekommunikation wird ermächtigt,\\ndurch Rechtsverordnung ohne Zustimmung des Bundesrates die\\nVollzugsordnungen vom 15. Dezember 1989 zu den in Artikel 1 unter\\nNummer 3 bis 7 genannten Verträgen sowie Änderungen, die der\\nVollzugsrat des Weltpostvereins vor Zusammentreten des nächsten\\nWeltpostkongresses zu diesen Vollzugsordnungen beschließt, in Kraft zu\\nsetzen.\", \"snippets\": [{\"snippet_id\": \"STATUTE::WPostVtr1989G::Art.2::AbsNA::6e5e532e::2::S1\", \"text\": \"Der Bundesminister für Post und Telekommunikation wird ermächtigt,\\ndurch Rechtsverordnung ohne Zustimmung des Bundesrates die\\nVollzugsordnungen vom 15.\", \"span\": {\"start\": 0, \"end\": 151}}, {\"snippet_id\": \"STATUTE::WPostVtr1989G::Art.2::AbsNA::6e5e532e::2::S2\", \"text\": \"Dezember 1989 zu den in Artikel 1 unter\\nNummer 3 bis 7 genannten Verträgen sowie Änderungen, die der\\nVollzugsrat des Weltpostvereins vor Zusammentreten des nächsten\\nWeltpostkongresses zu diesen Vollzugsordnungen beschließt, in Kraft zu\\nsetzen.\", \"span\": {\"start\": 152, \"end\": 395}}], \"source\": {\"source_id\": \"SRC::gesetze::index.md::6e5e532e2802\", \"path\": \"/content/lawRAG/data_docs/raw/gesetze-master/w/wpostvtr1989g/index.md\", \"sha256\": \"6e5e532e2802d826ab3b24b152eda7c8140913ede9596cf5c98bb331ded30865\"}}\n"
     ]
    }
   ],
   "source": [
    "!MAX_MD_FILES=50 python -m src.legalrag.ingest\n",
    "!wc -l data_docs/processed/statute_chunks.jsonl\n",
    "!head -n 2 data_docs/processed/statute_chunks.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82db298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | RUNNING FILE: /content/lawRAG/src/legalrag/ingest.py\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | RAW_MD_DIR(raw)='data_docs/raw/gesetze-master' | abs=/content/lawRAG/data_docs/raw/gesetze-master | exists=True\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | Loaded md_files from filelist: data_docs/processed/md_filelist.txt | count=6637\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | Sample md: /content/lawRAG/data_docs/raw/gesetze-master/README.md\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | After exclude sample md: /content/lawRAG/data_docs/raw/gesetze-master/README.md\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | RAW_MD_DIR=data_docs/raw/gesetze-master | md_files=6637\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | After exclude filter: md_files=6636\n",
      "2026-01-12 22:55:48 | INFO | legalrag.ingest | After exclude sample md: /content/lawRAG/data_docs/raw/gesetze-master/w/wzg_4appabek/index.md\n",
      "2026-01-12 22:55:49 | INFO | legalrag.ingest | Progress: 200/6636 files | sources=200 chunks=4607\n",
      "2026-01-12 22:55:50 | INFO | legalrag.ingest | Progress: 400/6636 files | sources=400 chunks=9792\n",
      "2026-01-12 22:55:51 | INFO | legalrag.ingest | Progress: 800/6636 files | sources=800 chunks=23318\n",
      "2026-01-12 22:55:52 | INFO | legalrag.ingest | Progress: 1000/6636 files | sources=1000 chunks=30898\n",
      "2026-01-12 22:55:54 | INFO | legalrag.ingest | Progress: 1400/6636 files | sources=1400 chunks=52333\n",
      "2026-01-12 22:55:54 | INFO | legalrag.ingest | Progress: 1600/6636 files | sources=1600 chunks=60486\n",
      "2026-01-12 22:55:55 | INFO | legalrag.ingest | Progress: 1800/6636 files | sources=1800 chunks=73722\n",
      "2026-01-12 22:55:56 | INFO | legalrag.ingest | Progress: 2200/6636 files | sources=2200 chunks=83967\n",
      "2026-01-12 22:55:57 | INFO | legalrag.ingest | Progress: 2400/6636 files | sources=2400 chunks=90876\n",
      "2026-01-12 22:55:58 | INFO | legalrag.ingest | Progress: 2600/6636 files | sources=2600 chunks=103669\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/content/lawRAG/src/legalrag/ingest.py\", line 347, in <module>\n",
      "  File \"/content/lawRAG/src/legalrag/ingest.py\", line 334, in ingest_all\n",
      "    ch_out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/encoder.py\", line 200, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/encoder.py\", line 258, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "1162 data_docs/processed/statute_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python -m src.legalrag.ingest\n",
    "!wc -l data_docs/processed/statute_chunks.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7304217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 22:56:09 | INFO | legalrag.index | Loaded JSONL chunks=1162 (bad=0)\n",
      "2026-01-12 22:56:09 | INFO | legalrag.index | Loaded chunks: 1162\n",
      "2026-01-12 22:56:09 | INFO | legalrag.index | BM25 built in 0.08s\n",
      "2026-01-12 22:56:09 | INFO | legalrag.index | Doc store saved: data_indexes/doc_store.jsonl\n",
      "2026-01-12 22:56:12 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
      "2026-01-12 22:56:15.421384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768258575.449636    4697 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768258575.458553    4697 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768258575.476320    4697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768258575.476354    4697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768258575.476359    4697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768258575.476364    4697 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-12 22:56:15.481064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-12 22:56:20 | INFO | datasets | TensorFlow version 2.19.0 available.\n",
      "2026-01-12 22:56:20 | INFO | datasets | JAX version 0.7.2 available.\n",
      "2026-01-12 22:56:21 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cuda:0\n",
      "2026-01-12 22:56:21 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Batches: 100% 19/19 [00:02<00:00,  8.25it/s]\n",
      "2026-01-12 22:56:28 | INFO | legalrag.index | Embeddings saved.\n",
      "2026-01-12 22:56:28 | WARNING | legalrag.index | FAISS unavailable: No module named 'faiss'\n",
      "total 4.9M\n",
      "drwxr-xr-x  2 root root 4.0K Jan 12 22:53 .\n",
      "drwxr-xr-x 10 root root 4.0K Jan 12 22:55 ..\n",
      "-rw-r--r--  1 root root 842K Jan 12 22:56 bm25_index.pkl\n",
      "-rw-r--r--  1 root root 2.3M Jan 12 22:56 doc_store.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Jan 12 22:56 embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "!python -m src.legalrag.index\n",
    "!ls -lah data_indexes | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82db9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs 0\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: 'Retriever' object has no attribute 'query'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from src.legalrag.retrieve import Retriever; r=Retriever(); print('docs', len(r.docs)); print(r.query('VwVfG § 35', top_k=3)[0])\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e7537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/lawRAG\n",
      "total 60K\n",
      "drwxr-xr-x 10 root root 4.0K Jan 12 22:55 .\n",
      "drwxr-xr-x  1 root root 4.0K Jan 12 22:44 ..\n",
      "drwxr-xr-x  4 root root 4.0K Jan 12 22:44 data_docs\n",
      "drwxr-xr-x  2 root root 4.0K Jan 12 22:53 data_indexes\n",
      "-rw-r--r--  1 root root  324 Jan 12 22:55 .env\n",
      "-rw-r--r--  1 root root  206 Jan 12 22:44 .env.example\n",
      "drwxr-xr-x 29 root root 4.0K Jan 12 22:54 gesetze\n",
      "drwxr-xr-x  8 root root 4.0K Jan 12 22:44 .git\n",
      "-rw-r--r--  1 root root  188 Jan 12 22:44 .gitignore\n",
      "drwxr-xr-x  6 root root 4.0K Jan 12 22:53 lawRAG\n",
      "-rw-r--r--  1 root root    0 Jan 12 22:44 pyproject.toml\n",
      "-rw-r--r--  1 root root    0 Jan 12 22:44 README.md\n",
      "-rw-r--r--  1 root root  219 Jan 12 22:44 requirements.txt\n",
      "drwxr-xr-x  2 root root 4.0K Jan 12 22:44 scripts\n",
      "drwxr-xr-x  3 root root 4.0K Jan 12 22:44 src\n",
      "drwxr-xr-x  2 root root 4.0K Jan 12 22:44 tests\n",
      "-rw-r--r--  1 root root 1018 Jan 12 22:44 Untitled-1.ipynb\n",
      "total 4.9M\n",
      "drwxr-xr-x  2 root root 4.0K Jan 12 22:53 .\n",
      "drwxr-xr-x 10 root root 4.0K Jan 12 22:55 ..\n",
      "-rw-r--r--  1 root root 842K Jan 12 22:56 bm25_index.pkl\n",
      "-rw-r--r--  1 root root 2.3M Jan 12 22:56 doc_store.jsonl\n",
      "-rw-r--r--  1 root root 1.8M Jan 12 22:56 embeddings.npy\n",
      "ls: cannot access 'data_index': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%cd /content/lawRAG\n",
    "!ls -lah\n",
    "!ls -lah data_indexes | head\n",
    "!ls -lah data_index   | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3067e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/lawRAG\n",
      "﻿# Optional overrides (defaults already set in src/legalrag/config.py)\n",
      "# RAW_MD_DIR=data_docs/raw/gesetze_md\n",
      "# PROCESSED_DIR=data_docs/processed\n",
      "# INDEX_DIR=data_indexes\n",
      "# RETRIEVAL_MODE=hybrid\n",
      "# TOP_K=8\n",
      "\n",
      "RAW_MD_DIR=data_docs/raw/gesetze-master\n",
      "PROCESSED_DIR=data_docs/processed\n",
      "INDEX_DIR=data_indexes\n",
      "RETRIEVAL_MODE=bm25\n"
     ]
    }
   ],
   "source": [
    "%cd /content/lawRAG\n",
    "!sed -n '1,200p' .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd042692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX_DIR = data_indexes\n"
     ]
    }
   ],
   "source": [
    "from src.legalrag.config import settings\n",
    "print(\"INDEX_DIR =\", settings.INDEX_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33773690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX_DIR = data_indexes\n",
      "exists = True\n",
      "files = [PosixPath('data_indexes/bm25_index.pkl'), PosixPath('data_indexes/doc_store.jsonl'), PosixPath('data_indexes/embeddings.npy')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "print(\"INDEX_DIR =\", settings.INDEX_DIR)\n",
    "print(\"exists =\", Path(settings.INDEX_DIR).exists())\n",
    "print(\"files =\", list(Path(settings.INDEX_DIR).glob(\"*\"))[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac0c49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX_DIR = data_indexes\n",
      "exists = True\n",
      "files = [PosixPath('data_indexes/bm25_index.pkl'), PosixPath('data_indexes/doc_store.jsonl'), PosixPath('data_indexes/embeddings.npy')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "print(\"INDEX_DIR =\", settings.INDEX_DIR)\n",
    "print(\"exists =\", Path(settings.INDEX_DIR).exists())\n",
    "print(\"files =\", list(Path(settings.INDEX_DIR).glob(\"*\"))[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b510513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Optional overrides (defaults already set in src/legalrag/config.py)\n",
      "# RAW_MD_DIR=data_docs/raw/gesetze_md\n",
      "# PROCESSED_DIR=data_docs/processed\n",
      "# INDEX_DIR=data_indexes\n",
      "# RETRIEVAL_MODE=hybrid\n",
      "# TOP_K=8\n",
      "\n",
      "RAW_MD_DIR=data_docs/raw/gesetze-master\n",
      "PROCESSED_DIR=data_docs/processed\n",
      "INDEX_DIR=data_indexes\n",
      "RETRIEVAL_MODE=bm25\n"
     ]
    }
   ],
   "source": [
    "!sed -n '1,200p' .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbfce589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote fixed doc_store lines: 1 -> data_indexes/doc_store.fixed.jsonl\n",
      "fixed size_MB: 0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "src = Path(settings.INDEX_DIR) / \"doc_store.jsonl\"   # your current (bad) doc_store\n",
    "dst = Path(settings.INDEX_DIR) / \"doc_store.fixed.jsonl\"\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "\n",
    "def iter_objects_from_text_stream(f):\n",
    "    buf = \"\"\n",
    "    while True:\n",
    "        chunk = f.read(1024 * 1024)  # 1MB\n",
    "        if not chunk:\n",
    "            break\n",
    "        buf += chunk\n",
    "\n",
    "        while True:\n",
    "            s = buf.lstrip()\n",
    "            if not s:\n",
    "                buf = \"\"\n",
    "                break\n",
    "            try:\n",
    "                obj, idx = decoder.raw_decode(s)\n",
    "                yield obj\n",
    "                buf = s[idx:]\n",
    "            except json.JSONDecodeError:\n",
    "                # need more data\n",
    "                buf = s\n",
    "                break\n",
    "\n",
    "    # EOF: attempt one last drain; ignore trailing partial JSON\n",
    "    s = buf.lstrip()\n",
    "    while s:\n",
    "        try:\n",
    "            obj, idx = decoder.raw_decode(s)\n",
    "            yield obj\n",
    "            s = s[idx:].lstrip()\n",
    "        except json.JSONDecodeError:\n",
    "            # trailing partial -> drop\n",
    "            break\n",
    "\n",
    "good = 0\n",
    "with src.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f_in, dst.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "    for obj in iter_objects_from_text_stream(f_in):\n",
    "        f_out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        good += 1\n",
    "\n",
    "print(\"Wrote fixed doc_store lines:\", good, \"->\", dst)\n",
    "print(\"fixed size_MB:\", round(dst.stat().st_size / 1024 / 1024, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec19cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e69caeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed docs written: 1\n",
      "output: data_indexes/doc_store.fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "src = Path(\"data_indexes/doc_store.jsonl\")\n",
    "dst = Path(\"data_indexes/doc_store.fixed.jsonl\")\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "buf = \"\"\n",
    "n = 0\n",
    "\n",
    "with src.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f_in, dst.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "    while True:\n",
    "        chunk = f_in.read(1024 * 1024)\n",
    "        if not chunk:\n",
    "            break\n",
    "        buf += chunk\n",
    "\n",
    "        while True:\n",
    "            s = buf.lstrip()\n",
    "            if not s:\n",
    "                buf = \"\"\n",
    "                break\n",
    "            try:\n",
    "                obj, idx = decoder.raw_decode(s)\n",
    "            except json.JSONDecodeError:\n",
    "                buf = s\n",
    "                break\n",
    "\n",
    "            f_out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            n += 1\n",
    "            buf = s[idx:]\n",
    "\n",
    "print(\"fixed docs written:\", n)\n",
    "print(\"output:\", dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2191c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_store.jsonl replaced with fixed version\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "(idx / \"doc_store.jsonl\").unlink(missing_ok=True)\n",
    "(idx / \"doc_store.fixed.jsonl\").rename(idx / \"doc_store.jsonl\")\n",
    "print(\"doc_store.jsonl replaced with fixed version\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7e13fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-65772094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Was regelt § 433 BGB?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hits =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "q = \"Was regelt § 433 BGB?\"\n",
    "hits = r.search(q, top_k=5)\n",
    "\n",
    "print(\"hits =\", len(hits))\n",
    "for i, h in enumerate(hits, 1):\n",
    "    print(\n",
    "        f\"{i:02d}. score={h.get('score'):.4f} | {h.get('citation')} | {h.get('path')} | {h.get('chunk_id')}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d6c14b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_MD_DIR = data_docs/raw/gesetze-master | exists = True\n",
      "PROCESSED_DIR = data_docs/processed | exists = True\n",
      "INDEX_DIR = data_indexes | exists = True\n",
      "Index files: ['bm25_index.pkl', 'doc_store.jsonl', 'embeddings.npy']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "print(\"RAW_MD_DIR =\", settings.RAW_MD_DIR, \"| exists =\", Path(settings.RAW_MD_DIR).exists())\n",
    "print(\"PROCESSED_DIR =\", settings.PROCESSED_DIR, \"| exists =\", Path(settings.PROCESSED_DIR).exists())\n",
    "print(\"INDEX_DIR =\", settings.INDEX_DIR, \"| exists =\", Path(settings.INDEX_DIR).exists())\n",
    "\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "print(\"Index files:\", [p.name for p in idx.glob(\"*\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "094307f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve module file = /content/lawRAG/src/legalrag/retrieve.py\n",
      "Has Retriever.size ? False\n",
      "class Retriever:\n",
      "    def __init__(self) -> None:\n",
      "        self.index_dir = settings.INDEX_DIR\n",
      "        self.doc_store_path = os.path.join(self.index_dir, \"doc_store.jsonl\")\n",
      "        self.bm25_path = os.path.join(self.index_dir, \"bm25_index.pkl\")\n",
      "        self.emb_path = os.path.join(self.index_dir, \"embeddings.npy\")\n",
      "\n",
      "        if not os.path.exists(self.doc_store_path):\n",
      "            raise FileNotFoundError(f\"Missing doc_store: {self.doc_store_path}\")\n",
      "        if not os.path.exists(self.bm25_path):\n",
      "            raise FileNotFoundError(f\"Missing bm25_index: {self.bm25_path}\")\n",
      "\n",
      "        self.docs = _load_d\n"
     ]
    }
   ],
   "source": [
    "import src.legalrag.retrieve as retrieve\n",
    "import inspect\n",
    "\n",
    "print(\"retrieve module file =\", retrieve.__file__)\n",
    "print(\"Has Retriever.size ?\", hasattr(retrieve.Retriever, \"size\"))\n",
    "\n",
    "# Optional: show the first lines of the class source to confirm what you're running\n",
    "print(inspect.getsource(retrieve.Retriever)[:600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "830bdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: /content/lawRAG\n",
      "sys.path[0]: /content/lawRAG\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "project_root = Path(os.getcwd()).resolve()   # should be /content/lawRAG\n",
    "print(\"project_root:\", project_root)\n",
    "\n",
    "# Put project root at the FRONT of sys.path\n",
    "if str(project_root) in sys.path:\n",
    "    sys.path.remove(str(project_root))\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"sys.path[0]:\", sys.path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b8aa7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting: /content/lawRAG/src/legalrag/retrieve.py\n",
      "Wrote bytes: 49\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import src.legalrag.retrieve as m\n",
    "\n",
    "target = Path(m.__file__)\n",
    "print(\"Overwriting:\", target)\n",
    "\n",
    "UPDATED = r'''<PASTE THE FULL UPDATED retrieve.py CONTENT HERE>'''\n",
    "# IMPORTANT: paste the full file content between the triple quotes above\n",
    "\n",
    "target.write_text(UPDATED, encoding=\"utf-8\")\n",
    "print(\"Wrote bytes:\", target.stat().st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14a93966",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (retrieve.py, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipython-input-882203803.py\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    importlib.reload(m)\n",
      "  File \u001b[1;32m\"/usr/lib/python3.12/importlib/__init__.py\"\u001b[0m, line \u001b[1;32m131\u001b[0m, in \u001b[1;35mreload\u001b[0m\n    _bootstrap._exec(spec, module)\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m866\u001b[0m, in \u001b[1;35m_exec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m995\u001b[0m, in \u001b[1;35mexec_module\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1133\u001b[0m, in \u001b[1;35mget_code\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1063\u001b[0m, in \u001b[1;35msource_to_code\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0;36m, line \u001b[0;32m488\u001b[0;36m, in \u001b[0;35m_call_with_frames_removed\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/content/lawRAG/src/legalrag/retrieve.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <PASTE THE FULL UPDATED retrieve.py CONTENT HERE>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.legalrag.retrieve as m\n",
    "importlib.reload(m)\n",
    "\n",
    "print(\"retrieve module file:\", m.__file__)\n",
    "print(\"Has Retriever.size ?\", hasattr(m.Retriever, \"size\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41216243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored: /content/lawRAG/src/legalrag/retrieve.py bytes= 7565\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fixed = r'''# src/legalrag/retrieve.py\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from .config import settings\n",
    "from .textnorm import tokenize\n",
    "\n",
    "log = logging.getLogger(\"legalrag.retrieve\")\n",
    "\n",
    "\n",
    "def _load_doc_store(path: str) -> List[Dict[str, Any]]:\n",
    "    docs: List[Dict[str, Any]] = []\n",
    "    bad = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                docs.append(json.loads(line))\n",
    "            except Exception:\n",
    "                bad += 1\n",
    "                continue\n",
    "    log.info(\"Loaded doc_store docs=%d (bad=%d)\", len(docs), bad)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def _parse_query_constraints(q: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract constraints like:\n",
    "      - paragraf: from \"§ 433\"\n",
    "      - law_short: from \"BGB\"\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    qq = q.strip()\n",
    "    m = re.search(r\"§\\\\s*([0-9]+[a-zA-Z]?)\", qq)\n",
    "    paragraf = m.group(1) if m else None\n",
    "\n",
    "    law_short = None\n",
    "    m2 = re.search(r\"\\\\b(BGB|StGB|ZPO|VwGO|VwVfG|GG|HGB)\\\\b\", qq)\n",
    "    if m2:\n",
    "        law_short = m2.group(1)\n",
    "\n",
    "    return {\"paragraf\": paragraf, \"law_short\": law_short}\n",
    "\n",
    "\n",
    "def _match_constraints(chunk: Dict[str, Any], constraints: Dict[str, Optional[str]]) -> bool:\n",
    "    p = constraints.get(\"paragraf\")\n",
    "    law = constraints.get(\"law_short\")\n",
    "\n",
    "    if not p and not law:\n",
    "        return True\n",
    "\n",
    "    citation = (chunk.get(\"citation\") or {})\n",
    "    locator = (citation.get(\"locator\") or {})\n",
    "    canonical = (citation.get(\"canonical\") or \"\")\n",
    "\n",
    "    if p:\n",
    "        loc_p = locator.get(\"paragraf\") or locator.get(\"artikel\")\n",
    "        if loc_p is not None:\n",
    "            if str(loc_p) != str(p):\n",
    "                return False\n",
    "        else:\n",
    "            if f\"§ {p}\" not in canonical and f\"Art. {p}\" not in canonical:\n",
    "                return False\n",
    "\n",
    "    if law:\n",
    "        law_short = (chunk.get(\"law\") or {}).get(\"short\")\n",
    "        if law_short:\n",
    "            if law_short != law:\n",
    "                return False\n",
    "        else:\n",
    "            if law not in canonical:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self) -> None:\n",
    "        self.index_dir = settings.INDEX_DIR\n",
    "        self.doc_store_path = os.path.join(self.index_dir, \"doc_store.jsonl\")\n",
    "        self.bm25_path = os.path.join(self.index_dir, \"bm25_index.pkl\")\n",
    "        self.emb_path = os.path.join(self.index_dir, \"embeddings.npy\")\n",
    "\n",
    "        if not os.path.exists(self.doc_store_path):\n",
    "            raise FileNotFoundError(f\"Missing doc_store: {self.doc_store_path}\")\n",
    "        if not os.path.exists(self.bm25_path):\n",
    "            raise FileNotFoundError(f\"Missing bm25_index: {self.bm25_path}\")\n",
    "\n",
    "        self.docs = _load_doc_store(self.doc_store_path)\n",
    "\n",
    "        with open(self.bm25_path, \"rb\") as f:\n",
    "            self.bm25 = pickle.load(f)\n",
    "\n",
    "        self.embeddings = None\n",
    "        if os.path.exists(self.emb_path):\n",
    "            try:\n",
    "                self.embeddings = np.load(self.emb_path).astype(np.float32)\n",
    "                log.info(\"Loaded embeddings: %s\", self.embeddings.shape)\n",
    "            except Exception as e:\n",
    "                log.warning(\"Failed to load embeddings.npy: %s\", e)\n",
    "\n",
    "    # --- Convenience methods expected by your API/tests ---\n",
    "    def size(self) -> int:\n",
    "        return len(self.docs)\n",
    "\n",
    "    def query(self, q: str, top_k: int = 8) -> List[Dict[str, Any]]:\n",
    "        return self.search(q, top_k=top_k)\n",
    "\n",
    "    # --- Core retrieval ---\n",
    "    def search(self, query: str, top_k: int = 8) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Returns list of hits:\n",
    "          [{ \"rank\": 1, \"score\": float, \"chunk\": <doc> }, ...]\n",
    "        Applies constraint filtering when query includes \"§ NNN\" and/or law short like \"BGB\".\n",
    "        \"\"\"\n",
    "        mode = (settings.RETRIEVAL_MODE or \"bm25\").lower()\n",
    "        constraints = _parse_query_constraints(query)\n",
    "\n",
    "        if mode == \"emb\" and self.embeddings is not None:\n",
    "            return self._search_embeddings(query, top_k=top_k, constraints=constraints)\n",
    "\n",
    "        if mode == \"hybrid\" and self.embeddings is not None:\n",
    "            return self._search_hybrid(query, top_k=top_k, constraints=constraints)\n",
    "\n",
    "        return self._search_bm25(query, top_k=top_k, constraints=constraints)\n",
    "\n",
    "    def _search_bm25(self, query: str, top_k: int, constraints: Dict[str, Optional[str]]) -> List[Dict[str, Any]]:\n",
    "        if not self.docs:\n",
    "            return []\n",
    "\n",
    "        q_tokens = tokenize(query)\n",
    "        scores = self.bm25.get_scores(q_tokens)  # np array\n",
    "\n",
    "        pre_k = min(len(self.docs), max(top_k * 10, 50))\n",
    "        idxs = np.argpartition(-scores, pre_k - 1)[:pre_k]\n",
    "        idxs = idxs[np.argsort(-scores[idxs])]\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for i in idxs:\n",
    "            chunk = self.docs[int(i)]\n",
    "            if not _match_constraints(chunk, constraints):\n",
    "                continue\n",
    "            results.append({\"rank\": len(results) + 1, \"score\": float(scores[int(i)]), \"chunk\": chunk})\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        return results\n",
    "\n",
    "    def _search_embeddings(self, query: str, top_k: int, constraints: Dict[str, Optional[str]]) -> List[Dict[str, Any]]:\n",
    "        if self.embeddings is None or not self.docs:\n",
    "            return []\n",
    "\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        model = SentenceTransformer(settings.EMBED_MODEL_NAME)\n",
    "        q_emb = model.encode([query], normalize_embeddings=True)\n",
    "        q_emb = np.asarray(q_emb, dtype=np.float32)[0]\n",
    "\n",
    "        scores = self.embeddings @ q_emb  # cosine if embeddings normalized\n",
    "        pre_k = min(len(self.docs), max(top_k * 10, 50))\n",
    "        idxs = np.argpartition(-scores, pre_k - 1)[:pre_k]\n",
    "        idxs = idxs[np.argsort(-scores[idxs])]\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for i in idxs:\n",
    "            chunk = self.docs[int(i)]\n",
    "            if not _match_constraints(chunk, constraints):\n",
    "                continue\n",
    "            results.append({\"rank\": len(results) + 1, \"score\": float(scores[int(i)]), \"chunk\": chunk})\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        return results\n",
    "\n",
    "    def _search_hybrid(self, query: str, top_k: int, constraints: Dict[str, Optional[str]]) -> List[Dict[str, Any]]:\n",
    "        if self.embeddings is None or not self.docs:\n",
    "            return self._search_bm25(query, top_k=top_k, constraints=constraints)\n",
    "\n",
    "        bm = self._search_bm25(query, top_k=max(top_k * 10, 50), constraints={\"paragraf\": None, \"law_short\": None})\n",
    "        em = self._search_embeddings(query, top_k=max(top_k * 10, 50), constraints={\"paragraf\": None, \"law_short\": None})\n",
    "\n",
    "        scores: Dict[str, float] = {}\n",
    "        for rank, h in enumerate(bm, start=1):\n",
    "            cid = (h.get(\"chunk\") or {}).get(\"chunk_id\")\n",
    "            if cid:\n",
    "                scores[cid] = scores.get(cid, 0.0) + 1.0 / rank\n",
    "        for rank, h in enumerate(em, start=1):\n",
    "            cid = (h.get(\"chunk\") or {}).get(\"chunk_id\")\n",
    "            if cid:\n",
    "                scores[cid] = scores.get(cid, 0.0) + 1.0 / rank\n",
    "\n",
    "        merged = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        id_to_doc = {d.get(\"chunk_id\"): d for d in self.docs}\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for cid, sc in merged:\n",
    "            chunk = id_to_doc.get(cid)\n",
    "            if not chunk:\n",
    "                continue\n",
    "            if not _match_constraints(chunk, constraints):\n",
    "                continue\n",
    "            results.append({\"rank\": len(results) + 1, \"score\": float(sc), \"chunk\": chunk})\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "        return results\n",
    "'''\n",
    "\n",
    "path = Path(\"/content/lawRAG/src/legalrag/retrieve.py\")\n",
    "path.write_text(fixed, encoding=\"utf-8\")\n",
    "print(\"Restored:\", path, \"bytes=\", path.stat().st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b43a337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX_DIR: data_indexes\n",
      "doc_store exists: True size_MB: 2.3\n",
      "bm25 exists: True size_MB: 0.8\n",
      "doc_store sample parseable: 1162 bad: 0\n",
      "bm25 corpus_size: 1162\n"
     ]
    }
   ],
   "source": [
    "import pickle, json\n",
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "doc_path = idx / \"doc_store.jsonl\"\n",
    "bm_path  = idx / \"bm25_index.pkl\"\n",
    "\n",
    "print(\"INDEX_DIR:\", idx)\n",
    "print(\"doc_store exists:\", doc_path.exists(), \"size_MB:\", round(doc_path.stat().st_size/1024/1024, 1))\n",
    "print(\"bm25 exists:\", bm_path.exists(), \"size_MB:\", round(bm_path.stat().st_size/1024/1024, 1))\n",
    "\n",
    "# Count how many JSON lines are actually parseable (sample)\n",
    "good = bad = 0\n",
    "with doc_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            json.loads(line)\n",
    "            good += 1\n",
    "        except Exception:\n",
    "            bad += 1\n",
    "        if i >= 5000:\n",
    "            break\n",
    "print(\"doc_store sample parseable:\", good, \"bad:\", bad)\n",
    "\n",
    "with bm_path.open(\"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "print(\"bm25 corpus_size:\", getattr(bm25, \"corpus_size\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fbab59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: data_docs/processed/statute_chunks.jsonl exists: True\n",
      "DST: data_indexes/doc_store.jsonl\n",
      "Rebuilt doc_store lines: 1162 | size_MB: 2.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "src = Path(settings.PROCESSED_DIR) / \"statute_chunks.jsonl\"\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "idx.mkdir(parents=True, exist_ok=True)\n",
    "dst = idx / \"doc_store.jsonl\"\n",
    "\n",
    "print(\"SRC:\", src, \"exists:\", src.exists())\n",
    "print(\"DST:\", dst)\n",
    "\n",
    "n = 0\n",
    "with src.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f_in, dst.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)  # validate\n",
    "        f_out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        n += 1\n",
    "\n",
    "print(\"Rebuilt doc_store lines:\", n, \"| size_MB:\", round(dst.stat().st_size/1024/1024, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aea8ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs loaded: 1162\n",
      "BM25 rebuilt. corpus_size: 1162 | size_MB: 0.8\n"
     ]
    }
   ],
   "source": [
    "import json, pickle\n",
    "from pathlib import Path\n",
    "from rank_bm25 import BM25Okapi\n",
    "from src.legalrag.textnorm import tokenize\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "doc_path = idx / \"doc_store.jsonl\"\n",
    "bm_path  = idx / \"bm25_index.pkl\"\n",
    "\n",
    "docs = []\n",
    "with doc_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        docs.append(json.loads(line))\n",
    "\n",
    "print(\"docs loaded:\", len(docs))\n",
    "\n",
    "corpus_tokens = [tokenize(d.get(\"text\",\"\")) for d in docs]\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "with bm_path.open(\"wb\") as f:\n",
    "    pickle.dump(bm25, f)\n",
    "\n",
    "print(\"BM25 rebuilt. corpus_size:\", bm25.corpus_size, \"| size_MB:\", round(bm_path.stat().st_size/1024/1024, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d88f349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_store lines: 1162\n",
      "bm25 corpus_size: 1162\n",
      "MATCH: True\n"
     ]
    }
   ],
   "source": [
    "import json, pickle\n",
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "doc_path = idx / \"doc_store.jsonl\"\n",
    "bm_path  = idx / \"bm25_index.pkl\"\n",
    "\n",
    "# doc_store count\n",
    "doc_count = 0\n",
    "with doc_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for _ in f:\n",
    "        doc_count += 1\n",
    "\n",
    "with bm_path.open(\"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "\n",
    "print(\"doc_store lines:\", doc_count)\n",
    "print(\"bm25 corpus_size:\", bm25.corpus_size)\n",
    "print(\"MATCH:\", doc_count == bm25.corpus_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30b3f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: 1162\n",
      "hits: 5\n",
      "8.17603964788351 WBStiftG § 7 Abs. 3\n",
      "6.501378094784081 WährUmStAbschlG § 7\n",
      "5.7566293099420145 WBStiftG § 6 Abs. 4\n",
      "5.491328908491861 WBStiftG § 10 Abs. 1\n",
      "5.367642151794461 WpHG § 17 Abs. 1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.legalrag.retrieve as m\n",
    "importlib.reload(m)\n",
    "\n",
    "from src.legalrag.retrieve import Retriever\n",
    "r = Retriever()\n",
    "\n",
    "print(\"docs:\", len(r.docs))\n",
    "hits = r.search(\"Was regelt § 433 BGB?\", top_k=5)\n",
    "print(\"hits:\", len(hits))\n",
    "for h in hits:\n",
    "    print(h[\"score\"], h[\"chunk\"][\"citation\"][\"canonical\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82314f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/content/lawRAG\")  # repo root\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.legalrag.retrieve import Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05e34251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT exists: True\n",
      "src exists: True\n",
      "src in sys.path: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ROOT exists:\", Path(\"/content/lawRAG\").exists())\n",
    "print(\"src exists:\", Path(\"/content/lawRAG/src\").exists())\n",
    "print(\"src in sys.path:\", \"/content/lawRAG\" in sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da22b959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: 1162\n"
     ]
    }
   ],
   "source": [
    "from src.legalrag.retrieve import Retriever\n",
    "r = Retriever()\n",
    "print(\"docs:\", len(r.docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ee4f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd = /content/lawRAG\n",
      "expect repo root exists: True\n",
      "relative doc_store path = /content/lawRAG/data_indexes/doc_store.jsonl\n",
      "relative doc_store exists = True\n",
      "absolute doc_store exists = True | size_MB = 2.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"cwd =\", Path().resolve())\n",
    "print(\"expect repo root exists:\", Path(\"/content/lawRAG\").exists())\n",
    "\n",
    "p = Path(\"data_indexes/doc_store.jsonl\")\n",
    "print(\"relative doc_store path =\", p.resolve())\n",
    "print(\"relative doc_store exists =\", p.exists())\n",
    "\n",
    "p2 = Path(\"/content/lawRAG/data_indexes/doc_store.jsonl\")\n",
    "print(\"absolute doc_store exists =\", p2.exists(), \"| size_MB =\", round(p2.stat().st_size/1024/1024, 1) if p2.exists() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18f82d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd now = /content/lawRAG\n",
      "docs: 238737\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(\"/content/lawRAG\")\n",
    "print(\"cwd now =\", Path().resolve())\n",
    "\n",
    "from src.legalrag.retrieve import Retriever\n",
    "r = Retriever()\n",
    "print(\"docs:\", len(r.docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2d9e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX_DIR = data_indexes\n",
      "docs = 1162\n",
      "embeddings exists = True\n",
      "embeddings shape = (1162, 384)\n",
      "docs == embeddings rows ? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.legalrag.retrieve import Retriever\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "r = Retriever()\n",
    "\n",
    "print(\"INDEX_DIR =\", settings.INDEX_DIR)\n",
    "print(\"docs =\", len(r.docs))\n",
    "\n",
    "emb_path = Path(settings.INDEX_DIR) / \"embeddings.npy\"\n",
    "print(\"embeddings exists =\", emb_path.exists())\n",
    "\n",
    "if emb_path.exists():\n",
    "    emb = np.load(str(emb_path), mmap_mode=\"r\")\n",
    "    print(\"embeddings shape =\", emb.shape)\n",
    "    print(\"docs == embeddings rows ?\", len(r.docs) == emb.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01d55554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.17603964788351 WBStiftG § 7 Abs. 3\n",
      "6.501378094784081 WährUmStAbschlG § 7\n",
      "5.7566293099420145 WBStiftG § 6 Abs. 4\n",
      "5.491328908491861 WBStiftG § 10 Abs. 1\n",
      "5.367642151794461 WpHG § 17 Abs. 1\n"
     ]
    }
   ],
   "source": [
    "hits = r.search(\"Was regelt § 433 BGB?\", top_k=5)\n",
    "for h in hits:\n",
    "    print(h[\"score\"], h[\"chunk\"][\"citation\"][\"canonical\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aba9a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: 1162\n"
     ]
    }
   ],
   "source": [
    "from src.legalrag.retrieve import Retriever\n",
    "r = Retriever()\n",
    "print(\"docs:\", len(r.docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c3bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell is executing\n"
     ]
    }
   ],
   "source": [
    "print(\"cell is executing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.legalrag.config import settings\n",
    "\n",
    "print(\"INDEX_DIR =\", settings.INDEX_DIR)\n",
    "idx = Path(settings.INDEX_DIR)\n",
    "print(\"INDEX_DIR exists:\", idx.exists())\n",
    "print(\"Index files:\", sorted([p.name for p in idx.glob(\"*\")]))\n",
    "\n",
    "doc_store = idx / \"doc_store.jsonl\"\n",
    "bm25 = idx / \"bm25_index.pkl\"\n",
    "print(\"doc_store exists:\", doc_store.exists(), \"size_MB:\", round(doc_store.stat().st_size/1024/1024, 1) if doc_store.exists() else None)\n",
    "print(\"bm25 exists:\", bm25.exists(), \"size_MB:\", round(bm25.stat().st_size/1024/1024, 1) if bm25.exists() else None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
